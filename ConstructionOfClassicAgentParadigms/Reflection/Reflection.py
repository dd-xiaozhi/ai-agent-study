import sys
import os

# æ·»åŠ çˆ¶ç›®å½•åˆ°è·¯å¾„
sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..'))

from LLMClient import LLMClient
from Memory import Memory

INITIAL_PROMPT_TEMPLATE = """
    ä½ æ˜¯ä¸€ä½èµ„æ·±çš„ Python ä¸“å®¶ã€‚è¯·æ ¹æ®ä»¥ä¸‹è¦æ±‚ï¼Œç¼–å†™ä¸€ä¸ª Python å‡½æ•°ã€‚
    ä½ çš„ä»£ç å¿…é¡»è¦åŒ…å«å®Œæ•´çš„å‡½æ•°åã€æ–‡æ¡£å­—ç¬¦ä¸²ã€å¹¶éµå¾ª PEP 8 è§„èŒƒã€‚

    è¦æ±‚ï¼š{task}

    è¯·ç›´æ¥è¾“å‡ºä»£ç ï¼Œä¸è¦åŒ…å«ä»»ä½•é¢å¤–çš„è§£é‡Š
    """

REFLECTION_PROMPT_TEMPLATE = """
    ä½ æ˜¯ä¸€ä½æå…¶ä¸¥æ ¼çš„ä»£ç è¯„å®¡ä¸“å®¶å’Œèµ„æ·±ç®—æ³•å·¥ç¨‹å¸ˆï¼Œå¯¹ä»£ç çš„æ€§èƒ½æœ‰æè‡´çš„è¦æ±‚ã€‚
    ä½ çš„ä»»åŠ¡æ˜¯å®¡æŸ¥ä»¥ä¸‹ Python ä»£ç ï¼Œå¹¶ä¸“æ³¨äºæ‰¾å‡ºå…¶åœ¨ <strong>ç®—æ³•æ•ˆç‡</strong> ä¸Šçš„ä¸»è¦ç“¶é¢ˆã€‚

    # åŸå§‹ä»»åŠ¡ï¼š
    {task}

    # ä»£å®¡æŸ¥çš„ä»£ç ï¼š
    ```python
    {code}
    ```

    è¯·åˆ†æè¯¥ä»£ç çš„æ—¶é—´å¤æ‚åº¦ï¼Œå¹¶æ€è€ƒæ˜¯å¦å­˜åœ¨ä¸€ç§<strong>ç®—æ³•ä¸Šæ›´ä¼˜</strong>çš„è§£å†³æ–¹æ¡ˆæ¥æ˜¾è‘—æå‡æ€§èƒ½ã€‚
    å¦‚æœå­˜åœ¨ï¼Œè¯·æ¸…æ´—åœ°æŒ‡å‡ºå½“å‰ç®—æ³•çš„ä¸è¶³ï¼Œå¹¶æå‡ºå…·ä½“çš„ã€å¯è¡Œçš„æ”¹è¿›ç®—æ³•å»ºè®®ï¼ˆåˆ—å…¥ï¼Œä½¿ç”¨ç­›æ³•æ›¿ä»£è¯•é™¤æ³•ï¼‰ã€‚
    å¦‚æœä»£ç åœ¨ç®—æ³•å±‚é¢å·²ç»è¾¾æœ€ä¼˜ï¼Œæ‰èƒ½å›ç­”"æ— éœ€æ”¹è¿›"ã€‚

    è¯·ç›´æ¥è¾“å‡ºä½ çš„åé¦ˆï¼Œä¸è¦åŒ…å«ä»»ä½•é¢å¤–çš„è§£é‡Šã€‚
    """

REFINE_PROMPT_TEMPLATE = """
    ä½ æ˜¯ä¸€ä½èµ„æ·±çš„ Python ä¸“å®¶ã€‚ä½ æ­£åœ¨æ ¹æ®ä¸€ä½ä»£ç å®¡æŸ¥ä¸“å®¶çš„åé¦ˆæ¥ä¼˜åŒ–ä½ çš„ä»£ç ã€‚

    # åŸå§‹ä»»åŠ¡ï¼š
    {task}

    # ä½ ä¸Šä¸€è½®å°è¯•çš„ä»£ç ï¼š
    ```python
    {last_code_attempt}
    ```
    è¯„å®¡å‘˜çš„åé¦ˆï¼š
    {reviewer_feedback}

    è¯·æ ¹æ®è¯„å®¡å‘˜çš„åé¦ˆï¼Œç”Ÿæˆä¸€ä¸ªä¼˜åŒ–åçš„æ–°ç‰ˆæœ¬ä»£ç ã€‚
    ä½ çš„ä»£ç å¿…é¡»è¦åŒ…å«å®Œæ•´çš„å‡½æ•°åã€æ–‡æ¡£å­—ç¬¦ä¸²ã€å¹¶éµå¾ª PEP 8 è§„èŒƒã€‚
    è¯·ç›´æ¥è¾“å‡ºä¼˜åŒ–åçš„ä»£ç ï¼Œä¸éœ€è¦åŒ…å«ä»»ä½•é¢å¤–çš„è§£é‡Šã€‚
    """


class ReflectionAgent:

    def __init__(self, llm_client: LLMClient, max_iterations: int = 3):
        self.llm_client = llm_client
        self.max_iterations = max_iterations
        self.memory = Memory()

    def run(self, task: str):
        print(f"ğŸ” å¼€å§‹æ‰§è¡Œä»»åŠ¡: {task}")

        # -------------------- åˆå§‹æ‰§è¡Œ --------------------
        print("\n--- æ­£åœ¨è¿›è¡Œåˆå§‹å°è¯• ---")
        initial_prompt = INITIAL_PROMPT_TEMPLATE.format(task=task)
        initial_code = self._get_llm_response(initial_prompt)
        self.memory.add_record(record_type="execution", record_content=initial_code)

        # -------------------- è¿­ä»£æ‰§è¡Œï¼šåæ€ä¼˜åŒ– --------------------
        for i in range(self.max_iterations):
            print(f"\n--- æ­£åœ¨è¿›è¡Œç¬¬ {i + 1}/{self.max_iterations} è½®åæ€ä¼˜åŒ– ---")

            # a.åæ€
            print("\n-> æ­£åœ¨è¿›è¡Œåæ€...")
            last_code_attempt = self.memory.get_last_execution()
            reflection_prompt = REFLECTION_PROMPT_TEMPLATE.format(
                task=task, code=initial_code
            )
            reflection_feedback = self._get_llm_response(reflection_prompt)
            self.memory.add_record(
                record_type="reflection", record_content=reflection_feedback
            )

            # b.æ£€æŸ¥æ˜¯å¦éœ€è¦åœæ­¢
            if "æ— éœ€æ”¹è¿›" in reflection_feedback:
                print("âœ… ä»£ç åœ¨ç®—æ³•å±‚é¢å·²ç»è¾¾æœ€ä¼˜ï¼Œæ— éœ€ç»§ç»­ä¼˜åŒ–ï¼Œä»»åŠ¡å®Œæˆã€‚")
                break

            # c.ä¼˜åŒ–
            print("\n-> æ­£åœ¨è¿›è¡Œä¼˜åŒ–...")
            refine_prompt = REFINE_PROMPT_TEMPLATE.format(
                task=task,
                last_code_attempt=last_code_attempt,
                reviewer_feedback=reflection_feedback,
            )
            refined_code = self._get_llm_response(refine_prompt)
            self.memory.add_record(record_type="execution", record_content=refined_code)

        final_code = self.memory.get_last_execution()
        print(f"\nğŸ‰ æœ€ç»ˆæ‰§è¡Œç»“æœ: {final_code}")

    def _get_llm_response(self, prompt: str) -> str:
        """
        è°ƒç”¨å¤§æ¨¡å‹ï¼Œç”Ÿæˆå›ç­”
        """
        messages = [{"role": "user", "content": prompt}]

        return self.llm_client.generate(messages, stream=True) or ""


if __name__ == "__main__":
    llm_client = LLMClient(model="deepseek-chat")
    reflection_agent = ReflectionAgent(llm_client=llm_client)
    reflection_agent.run(task="ç¼–å†™ä¸€ä¸ªæ’åºç®—æ³•")
